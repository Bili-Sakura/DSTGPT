{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从零开始部署领域大模型应用——以DST-GPT为例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Bg Image](../assets/BG.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTALL PACKAGES\n",
    "# using conda env | python=3.8.18\n",
    "\n",
    "%pip install python-dotenv\n",
    "%pip install langchain==0.1.4\n",
    "%pip install langchain-openai==0.0.4\n",
    "\n",
    "# %pip install langchain-community==0.0.16\n",
    "# %pip install langchain-core==0.1.16\n",
    "# %pip install langsmith==0.0.87"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Quickstart | 🦜️🔗 Langchain](https://python.langchain.com/docs/get_started/quickstart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 由于我们使用的是OpenAI的服务，使用此应用时可能需要用到魔法  \n",
    "> 但如果你的OpenAI API key是中转类型的，可能不需要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 配置OpenAI API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Privacy\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Set up your API key from OpenAI\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "base_url=os.getenv('OPENAI_BASE_URL') \n",
    "\n",
    "# Print\n",
    "print(api_key)\n",
    "print(base_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 创建一个基本LLM应用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，我们可以初始化模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=api_key, # alias:api_key\n",
    "    base_url=\"\", # If you use proxy api key, set base_url as needed \n",
    "    model=\"gpt-3.5-turbo-0125\", \n",
    "    temperature=0.7, #default\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一旦您安装并初始化了您选择的LLM，我们就可以尝试使用它了！让我们问它 '你调用的是哪个基模型'。\n",
    "> 大模型具有随机性的特点，完全相同的输入可以得到不同（但相近）的回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    response = llm.invoke(\"你用的是哪个底座模型\")\n",
    "    print(f\"Response {i+1}: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 使用向量数据库外挂知识增强LLM在特定领域的能力——以Don't Starve Together(DST)为例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sup1](../assets/DST1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们向gpt-3.5-turbo-0125提一个关于DST这款游戏的问题，即“Don't Starve Together这款游戏截止2023年12月，在Steam平台上共有多少评论？”。由于gpt-3.5-turbo-0125的训练语料数据仅截止到2021年过，显然其目前无法回答这个问题。此外，此问题过于领域化，基座大模型训练语料基本不可能包含相关信息，我们这里通过向量数据库外挂知识的方式，增加LLM在特地领域的问答能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=llm.invoke(\"Don't Starve Together这款游戏截止2023年12月，在Steam平台上共有多少评论？\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述回答为大模型幻觉（Illusion）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了正确回答此问题，我们需要为LLM添加额外知识。我们可以通过`Retrieval`来做到这一点。当您有太多数据无法直接传递时，`Retrieval`很有用。然后，您可以使用`Retriever`仅获取最相关的片段并将其传递。\n",
    "在这个过程中，我们将从 `Retriever` 中查找相关文档，然后将它们传递到`Prompt`中。在本例中，我们将填充一个`vectorstore`并将其用作`Retriever`。有关 `vectorstore` 的更多信息，请参阅[此文档](https://python.langchain.com/docs/modules/data_connection/vectorstores)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我们需要加载要索引的数据。这里我们的数据来源于网页信息，为此，我们将使用 WebBaseLoader。这需要安装 BeautifulSoup："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之后，我们可以导入并使用 WebBaseLoader。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "dst_wiki_mainpage_url=\"https://dontstarve.fandom.com/wiki/Don%27t_Starve_Together#Return_of_Them\"\n",
    "loader = WebBaseLoader(dst_wiki_mainpage_url)\n",
    "\n",
    "docs = loader.load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs[0].page_content[100:180])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "清洗数据，去掉\"\\n\"和\"\\t\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the text of the first Document instance\n",
    "original_text = docs[0].page_content\n",
    "\n",
    "# Use the replace method to remove newline and tab characters\n",
    "cleaned_text = original_text.replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "\n",
    "# Update the text attribute of the Document instance\n",
    "docs[0].page_content = cleaned_text\n",
    "\n",
    "# Print the length of the cleaned text and the cleaned text itself for confirmation\n",
    "print(\"文档字符串长度为：\", len(docs[0].page_content))\n",
    "#print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "截取这个问题的回答相关信息源，打印。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = docs[0].page_content\n",
    "start_index = content.find(\"As of 2023-12-05 on Steam, DST had\")\n",
    "end_index = content.find(\"new development takes place almost exclusively in DST.\")\n",
    "relevant_content = content[start_index:end_index]\n",
    "print(relevant_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们需要将其索引到`vectorstore`中。这需要一些组件，即`embedding`和`vectorstore`。\n",
    "对于`embedding`，我们再次使用OpenAI的示例。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "确保您安装了“langchain_openai”包并设置了适当的环境变量（这些变量与 所需的环境变量相同LLM）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model= \"text-embedding-ada-002\",#default\n",
    "    openai_api_key=api_key, # alias:api_key\n",
    "    base_url=base_url, # If you use proxy api key, set base_url as needed \n",
    "    chunk_size=1000,#default\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们可以使用这个`embedding`将文档摄取到`vectorstore`中。为了简单起见，我们将使用一个简单的本地`vectorstore`: `Chroma`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我们需要为此安装所需的软件包："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们可以构建我们的索引："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "vector = Chroma.from_documents(\n",
    "    documents=docs, \n",
    "    embedding=embeddings,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vector.get()['ids']))\n",
    "print(type(vector))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们已经在`vectorstore`中索引了这些数据，我们将创建一个`retrieval chain`。该`chain`将接受传入的问题，查找相关文档，然后将这些文档与原始问题一起传递到一个LLM中，并要求它回答原始问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，让我们设置一个`chain`，它接受一个问题和检索到的文档并生成一个答案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(\n",
    "    llm=llm, \n",
    "    prompt=prompt,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们现在可以调用这个chain了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "response = retrieval_chain.invoke({\"input\": \"DST这款游戏截止2023年12月，在Steam平台上共有多少评论？\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述回答完全正确 参照[Wiki Fandom](https://dontstarve.fandom.com/wiki/Don%27t_Starve_Together#Return_of_Them)  \n",
    "![DST2.png](../assets/DST2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 使用异步调用llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考[langchain官方文档](https://python.langchain.com/docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=await llm.ainvoke(\"你用的什么基模型\")\n",
    "print(response.content)\n",
    "print(type(response.content))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DST-GPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
